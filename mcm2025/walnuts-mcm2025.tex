\documentclass[10pt]{report}

\usepackage{stan-talks}

\begin{document}
\sf%
\vspace*{-12pt}
%
\noindent
\spc{\Huge\bfseries \color{MidnightBlue}{WALNUTS:}}
\\[6pt]
\spc{\LARGE\bfseries \color{MidnightBlue}{Per-leapfrog step size
    adaptation for HMC}}
\\[16pt]
\noindent
\spc{\Large\bfseries \color{MidnightBlue}{Bob Carpenter}}
\\[6pt]
\spc{\large Center for Computational Mathematics}
\\
\spc{\large Flatiron Institute}
\\[12pt]
\vfill
\hfill
\includegraphics[height=0.35in]{img/fi-logo.png}
\qquad
\includegraphics[height=0.5in]{img/stan-logo.png}

\sld{Bayesian statistics with Monte Carlo}
\begin{itemize}
\item Bayesian \myemph{posterior inference} given data $y$ with parameters $\theta$:
\begin{eqnarray*}
  \widehat{\theta} & = & \mathbb{E}[\Theta \mid y]
  \\
  \Pr[E \mid y] & = & \mathbb{E}[\textrm{I}_E(\Theta) \mid y]
  \\
  p(\widetilde{y} \mid y) & = & \mathbb{E}[p(\widetilde{y} \mid
                                \Theta) \mid y]
\end{eqnarray*}
\item Expectations involve \myemph{high-dimensional integrals}.
  $$ \mathbb{E}[f(\Theta) \mid y] = \int_\Theta f(\theta) p(\theta
  \mid y) \textrm{d}\theta. $$
\item (Markov chain) \myemph{Monte Carlo solves integrals} with $\theta^{(m)} \sim p(\theta \mid y).$
  $$ \mathbb{E}[f(\Theta) \mid y] \approx \frac{1}{M} \sum_{m=1}^M
  f(\theta^{(m)}) $$
\end{itemize}

\sld{Why NUTS?}
%
\begin{itemize}
\item Hamiltonian Monte Carlo (HMC, 1985), a Markov chain Monte
  Carlo (MCMC) method, \myemph{scales well in dimension}.
  \begin{subitemize}
  \item \myemph{integrated autocorrelation time}: $\mathcal{O}\left(D^{1/4}\right)$
   \qquad \myemph{memory}: $\mathcal{O}(D)$
  \item Hamiltonian \myemph{gradient flow} overcomes diffusive random walk behavior.
  \end{subitemize}
\item Why wasn't HMC in wide use until the \myemph{no-U-turn sampler}
  (NUTS, 2012)?
  \vspace*{-8pt}
  \begin{subitemize}
  \item NUTS in Stan, PyMC, NumPyro, Turing.jl, Blackjax, NIMBLE, ADMB, etc.
  \end{subitemize}
  \vfill
\item \myemph{Double whammy}:
  \begin{subitemize}
  \item HMC is hard to tune; NUTS brought \myemph{self tuning} and \myemph{general software}.
  \item gradients are hard for applications; \myemph{automatic differentiation} makes it easy.
  \end{subitemize}
\end{itemize}

\sld{What is HMC and why is it hard to tune?}
\begin{itemize}
\item Fictitious physical system with \myemph{position} $\theta \in
  \mathbb{R}^D$ and \myemph{momentum} $\rho \in \mathbb{R}^D$:
  \begin{subitemize}
  \item \myemph{Potential} energy: $U(\theta) = - \log p(\theta)$ for
    \myemph{target density} $p(\theta)$
  \item \myemph{Kinetic} energy: $K(\rho) - \log \textrm{normal}(\rho \mid 0, M)$
  \item \myemph{Hamiltonian} (total energy): $H(\theta, \rho) =
    U(\theta) + K(\rho)$, \quad $p(\theta, \rho) = \exp\left( -H(\theta, \rho)\right)$
  \end{subitemize}
\item \myemph{HMC iteration} from position $\theta$:
  \begin{subitemize}
  \item \myemph{Sample momentum}: $\rho \sim \textrm{normal}(0, M)$
  \item \myemph{Evolve Hamiltonian}: from $(\theta, \rho)$ for time $t$
    to $(\theta(t), \rho(t))$ and return $\theta(t)$.
  \end{subitemize}
\item In practice, can't solve exactly, so discretize.
  \begin{subitemize}
  \item \myemph{Simulate:} Take $L$ first-order steps of time $\epsilon = t / L$
    to $(\theta', \rho') \approx (\theta(t), \rho(t))$.
  \item \myemph{Metropolize:} Return $\theta'$ with probability $1 \wedge p(\theta',
      \rho') / p(\theta, \rho)$; \ else return $\theta$.
  \end{subitemize}
\end{itemize}

\sld{Harmonics in HMC}

\begin{minipage}[t]{0.7\textwidth}
  \includegraphics[width=\textwidth]{img/ess-per-leapfrog-jitter-hmc.png}
\end{minipage}%
\begin{minipage}[t]{0.29\textwidth}
  \null\vspace*{-2.5in}
  \begin{subitemize}
  \item \myemph{Target}: 1000-dim standard normal
  \item \myemph{y-axis} (log):  ESS; \\ \myemph{x-axis}: step size
  \item \myemph{blue}: $\mathbb{E}[\Theta]$; \\ \myemph{red}:
    $\mathbb{E}[\Theta^2]$
  \item \myemph{top}: HMC; \\ \myemph{bottom}: jittered time HMC
  \item \myemph{solid}:~HMC,\\ \myemph{dashed}: NUTS
  \end{subitemize}
\end{minipage}

\sld{Leapfrog integrator simulates trajectories}
\begin{itemize}
\item \myemph{Leapfrog step} from $(\theta(t), \rho(t))$ with step $\epsilon$, mass $M$
  \begin{eqnarray*}
    \rho(t + 1/2) & = & \rho(t) + \epsilon / 2 \cdot \nabla \log p(\theta(t))
    \\[4pt]
    \theta(t+1) & = & \theta(t) + \epsilon \cdot M^{-1} \cdot \rho(t + 1/2)
    \\[4pt]
    \rho(t + 1) & = & \rho(t + 1/2) + \epsilon / 2 \cdot \nabla \log p(\theta(t + 1)) 
  \end{eqnarray*}
\item \myemph{Stabilty} (i.e., preserving Hamiltonian)
  \begin{subitemize}
  \item \myemph{stable}: if $\epsilon < 2 / \sqrt{\lambda^\text{max}}$, where $\lambda^\textrm{max}$ is max eigenvalue of $\nabla \nabla^\top -\log p(\theta)$
  \item \myemph{local error}:  $\mathcal{O}(\epsilon^3)$
  \item \myemph{global error}: $\mathcal{O}(\epsilon^2)$
  \item $\epsilon < 1$ implies $\mathcal{O}(\epsilon^3) < \mathcal{O}(\epsilon^2)$
  \end{subitemize}
\end{itemize}

\sld{Multinomial HMC}
\begin{itemize}
\item \myemph{Each iteration} from previous position $\theta(0)$:
  \begin{subitemize}
  \item generate new momentum $\rho(0) \sim \textrm{normal}(0, M)$
  \item generate \myemph{random steps forward} $F \sim \textrm{uniform}(\{ 0, \ldots, L \})$
  \item take $B = L - F$ leapfrog \myemph{steps backward} and $F$ forward from
    $(\theta(0), \rho(0))$
  \item yields $(\theta(n), \rho(n))$ for $-B \leq n \leq F$
    (including initial)
  \item \myemph{randomly select next state} $(\theta(t+1), \rho(t+1))$ with probability \\ \myemph{proportional to density}, $\null \propto p(\theta(n), \rho(n))$
  \end{subitemize}
\item \myemph{No rejection}, \myemph{no harmonics}, \myemph{no bad
    luck} on final Hamiltonian
  \vfill
\item Straightforward to prove \myemph{detailed balance} (cf. our
  second \textit{GIST} paper)
\end{itemize}

\sld{Naive no-U-turn sampler (NUTS)}
\begin{itemize}
\item Go forward and backward and time at random
\item doubling number of steps each time
\item until a U-turn between ends or a sub-U-turn within doubled steps
\item select next state proportional to density (a la multinomial HMC)
  \vfill
\item Original NUTS: slice sample rather than multinomial sample
\end{itemize}

\sld{Biased-progressive NUTS}
\begin{itemize}
\item Keep a selected state, initially $z(0) = (\theta(0), \rho(0))$
\item At each doubling from $M$ to $2M$ states:
  \begin{subitemize}
    \item Extend $z(1), \ldots, z(M)$ to $z(1), \ldots, z(M), z(M+1), \ldots, z(2M)$
    \item Metropolis \myemph{probability of updating state}:
      $$
      1 \wedge
      \dfrac{p\!\left(z(M+1)\right) + \cdots + p\!\left(z(2M)\right)}
            {p\!\left(z(1)\right) + \cdots + p\!\left(z(M)\right)}
            $$
          \item If Hamiltonian simulation perfect, probability is 1.            
      \item If updating, \myemph{select new state} $z(m)$ 
        with probability \myemph{proportional to density},  $\propto p(z(m))$,
        from among candidate states $z(M+1), \ldots, z(2M)$.
  \end{subitemize}
\item Return final selected item.
\end{itemize}

\sld{QR code for C++ source, paper, and slides}
\\[8pt]
\spc
\noindent
\begin{minipage}[t]{0.35\textwidth}
\includegraphics[width=\textwidth]{img/qr-code.pdf}
\end{minipage}
\quad
\begin{minipage}[t]{0.6\textwidth}
  \vspace*{-1.55in}
  {\small Nawawf Bou-Rabee, Bob Carpenter, Tore S. Kleppe, and Sifan Liu.
      2025. \myemph{WALNUTS: The Within-Orbit Adaptive Leapfrog No-U-Turn
      Sampler}. \textit{arXiv} 2506.18746.}
      \\[12pt]
      \includegraphics[width=0.25\textwidth]{img/nawaf.jpg}
      \hfill
      \includegraphics[width=0.25\textwidth]{img/tore.png}
      \hfill
      \includegraphics[width=0.25\textwidth]{img/sifan.jpg}
\end{minipage}
\hfill
\null



\end{document}