\documentclass[9pt]{report}

\usepackage{talks}
\newcommand{\expect}[1]{\mathbb{E}\!\left[ #1 \right]}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\draw}[2]{#1^{(#2)}}
\usepackage{mathpazo}
\usepackage{sourcecodepro}

\begin{document}

\sf \mbox{}
\\[12pt]
\spc{{\LARGE\bfseries \color{MidnightBlue}{Language models for statisticians:}}
\\[8pt]
\spc{\Large\bfseries \color{MidnightBlue}{from $n$-grams to
    transformers to chatbots}}
\\[24pt]
\noindent 
\spc{\large\bfseries \color{MidnightBlue}{Bob Carpenter}}
\\[2pt]
\spc{\small Center for Computational Mathematics}
\\[2pt]
\spc{\small Flatiron Institute}
\vfill 
\noindent 
\spc{\footnotesize July 2023}
\hfill
\includegraphics[width=1.25in]{img/flatiron_logo.png}

\sld{What is a language model?}
\begin{itemize}
\item \myemph{Language} uses a \myemph{finite} number of
  symbols called \myemph{tokens}
  \subit{we assume a finite \myemph{token set} $\mathsf{Tok}$ of size $K$}
\item Tokens may be letters, words,
  sounds, syllables, etc.
  \begin{subitemize}
  \item letters or words traditional (e.g., Shannon 1948)
  \item GPT uses \myemph{sequences of letters} (average 1.5 tokens per
    English word)
  \end{subitemize}
\item Treat language as a \myemph{stochastic process}
  \subit{$Y = Y_1, Y_2, \ldots$ for random variables $Y_n \in T$}
\item Models typically \myemph{autoregressive}, predicting next
  word from previous
  $$
  p_{Y_{n+1} \mid Y_n, Y_{n-1}, \ldots, Y_1}\!\!\left(y_{n + 1} \mid y_n, y_{n-1}, \ldots, y_1\right).
  $$
\end{itemize}

\sld{$N$-gram language models \hfill \normalsize{(Shannon 1948)}}
\begin{itemize}
\item Assume language process is \myemph{order-$N$ Markov}
  \subit{$p(y_{n+1} \mid y_n, y_{n-1}, \ldots, y_1)
    = p(y_{n+1} \mid y_n, y_{n-1}).$}
\item Even \myemph{GPT-4 is Markovian}
  \begin{subitemize}
    \item but with $N = 4096$ (GPT-3) or $N = 8192$ (GPT-4)
    \item cf. a real computer is technically a finite-state machine
    \end{subitemize}
  \item Limited context allows relatively \myemph{efficient algorithms}
\end{itemize}
    
\sld{Shannon's $N$-gram models}
\begin{itemize}
\item \myemph{Claude Shannon}. 1948. \myemph{A Mathematical Theory of Communication.}
  \textit{Bell System Technical Journal}.
\item Shannon considered English \myemph{letters and words}, with $N = 1, 2, 3$
\item Shannon used \myemph{books of frequencies}
  \begin{subitemize}
  \item letter trigrams (1939 book)
  \item word bigrams (1923 book)
  \end{subitemize}
\item Through 2010s, people used \myemph{regularized MLE}
  \begin{subitemize}
  \item assigns \myemph{non-zero probability} to any sequence
  \item e.g., Laplace's law of succession
  \item can also use Bayesian models
  \end{subitemize}
\end{itemize}

\sld{Shannon's fit}
\begin{itemize}
\item MLE probabilities from compiled tables of letters (1923), words 
      (1939) 
    \subit{or, open books at random, find current context, generate 
        following word}
\item Shannon generated random examples
  \begin{subitemize}
    \item \myemph{Order 1, letters}: OCRO HLI RGWR NMIELWIS EU LL NBNESEBYA TH EEI ALHENHTTPA OOBTTVA
      NAH BRL.
    \item \myemph{Order 3, letters}: IN NO IST LAT WHEY CRATICT FROURE
      BIRS GROCID PONDENOME OF DEMONSTURES OF THE REPTAGIN IS
      REGOACTIONA OF
      % CRE.
    \item \myemph{Order 1, words}: REPRESENTING AND SPEEDILY IS AN
      GOOD APT OR COME CAN DIFFERENT NATURAL HERE HE THE A IN CAME THE
      TO OF TO EXPERT
      % GRAY COME TO FURNISHES THE LINE MESSAGE HAD BE THESE.
      \item \myemph{Order 2, words}: THE HEAD AND IN FRONTAL ATTACK ON
        AN ENGLISH WRITER THAT THE CHARACTER OF THIS POINT IS
        THEREFORE ANOTHER
        % METHOD FOR THE LETTERS THAT THE TIME OF WHO EVER TOLD THE PROBLEM FOR AN UNEXPECTED.
  \end{subitemize}
\end{itemize}
  

\end{document}
