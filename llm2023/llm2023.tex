\documentclass[9pt]{report}

\usepackage{talks}
\newcommand{\expect}[1]{\mathbb{E}\!\left[ #1 \right]}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\draw}[2]{#1^{(#2)}}
\usepackage{mathpazo}
\usepackage{sourcecodepro}

\begin{document}

\sf \mbox{}
\\[12pt]
\spc{{\LARGE\bfseries \color{MidnightBlue}{Language models for statisticians:}}
\\[8pt]
\spc{\Large\bfseries \color{MidnightBlue}{from $n$-grams to
    transformers to chatbots}}
\\[24pt]
\noindent 
\spc{\large\bfseries \color{MidnightBlue}{Bob Carpenter}}
\\[2pt]
\spc{\small Center for Computational Mathematics}
\\[2pt]
\spc{\small Flatiron Institute}
\vfill 
\noindent 
\spc{\footnotesize July 2023}
\hfill
\includegraphics[width=1.25in]{img/flatiron_logo.png}

\sld{What is a language model?}
\begin{itemize}
\item \myemph{Language} uses a \myemph{finite} number of
  symbols called \myemph{tokens}
  \subit{we assume a finite \myemph{token set} $\mathsf{Tok}$ of size $K$}
\item Tokens may be letters, words,
  sounds, syllables, etc.
  \begin{subitemize}
  \item letters or words traditional (e.g., Shannon 1948)
  \item GPT uses \myemph{sequences of letters} (average 1.5 tokens per
    English word)
  \end{subitemize}
\item Treat language as a \myemph{stochastic process}
  \subit{$Y = Y_1, Y_2, \ldots$ for random variables $Y_n \in T$}
\item Models typically \myemph{autoregressive}, predicting next
  word from previous
\end{itemize}

\sld{$K$-gram language models \hfill \normalsize{(Shannon 1948)}}
\begin{itemize}
\item Assume language process is \myemph{order-$K$ Markov}
  \begin{subitemize}
  \item tokens conditionally independent given previous $K$ tokens
    $$
    p(y_n \mid y_{n-1}, \ldots, y_1)
    = p(y_n \mid y_{n - 1}, \ldots, y_{n - K}).
    $$
  \end{subitemize}
\item Even \myemph{GPT is Markovian}
  \begin{subitemize}
    \item Chatbots use $N = 4096$ (GPT-3) or $N = 8192$ (GPT-4)
    \item API provides $N = 32,768$ GPT-4 and $N = 8192$ code specialization GPT-3
    \item cf. a real computer is technically a finite-state machine
    \end{subitemize}
  \item Limited context allows relatively \myemph{efficient algorithms}
\end{itemize}
    
\sld{Shannon's $K$-gram models}
\begin{itemize}
\item \myemph{Claude Shannon}. 1948. \myemph{A Mathematical Theory of Communication.}
  \textit{Bell System Technical Journal}.
\item Shannon considered English \myemph{letters} ($K = 1, 2, 3$) and
  words ($K = 1, 2$)
\item \myemph{What is English?}  How do we collect a \myemph{sample}?
\item Shannon used \myemph{books of frequencies}
  \begin{subitemize}
  \item \myemph{letter trigrams} (1939 book); \myemph{word bigrams} (1923 book)
  \end{subitemize}
\item Fit and inference usually \myemph{regularized MLE}
  \begin{subitemize}
  \item ensures \myemph{non-zero probability} for any sequence
  \item (non-parametric) \myemph{Bayes} usually too \myemph{expensive}
  \item often \myemph{heuristic driven} for efficiency (e.g., speech to text)
  \end{subitemize}
  
\end{itemize}

\sld{Shannon's fit}
\begin{itemize}
\item MLE probabilities from compiled tables of letters (1923), words 
      (1939) 
    \subit{or, open books at random, find current context, generate 
        following word}
\item Shannon generated random examples
  \begin{subitemize}
    \item \myemph{Order 1, letters}: OCRO HLI RGWR NMIELWIS EU LL NBNESEBYA TH EEI ALHENHTTPA OOBTTVA
      NAH BRL.
    \item \myemph{Order 3, letters}: IN NO IST LAT WHEY CRATICT FROURE
      BIRS GROCID PONDENOME OF DEMONSTURES OF THE REPTAGIN IS
      REGOACTIONA OF
      % CRE.
    \item \myemph{Order 1, words}: REPRESENTING AND SPEEDILY IS AN
      GOOD APT OR COME CAN DIFFERENT NATURAL HERE HE THE A IN CAME THE
      TO OF TO EXPERT
      % GRAY COME TO FURNISHES THE LINE MESSAGE HAD BE THESE.
      \item \myemph{Order 2, words}: THE HEAD AND IN FRONTAL ATTACK ON
        AN ENGLISH WRITER THAT THE CHARACTER OF THIS POINT IS
        THEREFORE ANOTHER
        % METHOD FOR THE LETTERS THAT THE TIME OF WHO EVER TOLD THE PROBLEM FOR AN UNEXPECTED.
  \end{subitemize}
\end{itemize}

\sld{Measuring accuracy with entropy}
\begin{itemize}
\item Accuracy of $K$-gram language model $p_Y$ measured with
  \myemph{entropy (rate)}
\item Given a random sequence $Y \in \textsf{Tok}^N,$ its
  \myemph{entropy} in \myemph{bits} (base 2) is
  $$
  \textrm{H}[Y]
  \ = \ \mathbb{E}\!\left[ \log_2 p_Y(Y) \right]
  \ = \ \sum_{y \in \textsf{Tok}^N} p_Y(y) \cdot \log_2 p_Y(y).
  $$
\item The \myemph{entropy rate} is average entropy per token, $\lim_{N \rightarrow \infty} \,
  \textrm{H}[Y] / N,$
\item The entropy rate for $K$-grams is given by \myemph{conditional entropy},
  \begin{align*}
    \textrm{H}[Y_n \mid Y_{n - 1}, \ldots, Y_{n -K}]
    \ &= \ \mathbb{E}\!\left[\, \log_2 p(Y_n \mid Y_{n - 1}, \ldots,
        Y_{n -K}) \, \right]
    \\[4pt]
    &= \ \sum_{y \in \textsf{Tok}^{K + 1}}
    \ p(y) \cdot \log_2 p(y_{K + 1} \mid y_{K}, \ldots, y_1).
  \end{align*}
\end{itemize}

\sld{Entropy and compression}
\begin{itemize}
\item Shannon (1948) introduced \myemph{information theory} to model
  signal compression and decompression for communication
\item Assume a language model with pmf $p_Y$
\item Given a string $y \in \textsf{Tok}^*,$ the language model can be
  used to \myemph{compress} $y$ to $\lceil \log_2 p_Y(y) \rceil$ bits
\item Can be done in practice using \myemph{arithmetic
    coding} (Rissanen 1976; Witten, Neal, Cleary 1987) using
  \myemph{GPT output}
\end{itemize}

\sld{GPT-3 training set sizes}
\begin{center}
  \begin{tabular}{r|r}
    \myemph{Source} & \myemph{Tokens} \\ \hline
    Common Crawl & 410 billion  \\
    Books2 & 55 billion \\
    WebText2 & 19 billion \\
    Books1 & 12 billion \\
    Wikipedia & 3 billion \\ \hline \hline
    & $\approx$ 500 billion
  \end{tabular}
\end{center}
\begin{itemize}
\item GPT-4 training set \myemph{size undisclosed}
  \subit{``Given both
the competitive landscape and the safety implications of large-scale models like GPT-4, this report
contains no further details about the architecture (including model size), hardware, training compute,
dataset construction, training method, or similar.''}
\end{itemize}


\end{document}
