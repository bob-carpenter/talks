\documentclass[10pt]{report}

\usepackage{stan-talks}

\begin{document}
\sf%
\mbox{ }
\\[12pt]
\spc{\LARGE\bfseries \color{MidnightBlue}{Taking Uncertainty Seriously}}
\\[4pt]
\spc{\large {for calibrated forecasting and decision making}}
\\[36pt]
\noindent
\spc{\Large\bfseries \color{MidnightBlue}{Bob Carpenter}}
\\[2pt]
\spc{Columbia University}
\vfill
\noindent
\spc{\small October 2019} \hfill
\hfill
\includegraphics[width=0.3in]{img/new-logo.png}

\mypart{}{Motivation}

\sld{Scientists}
\begin{itemize}
\item want to understand the world,
\item bring mechanistic theories,
\item bring data in the form of measurements, and
\item need to make predictions and evaluate theories.
\end{itemize}

\sld{Policymakers}
\begin{itemize}
\item control the social world,
\item bring hypotheses about interventions,
\item bring data in the form of measurements, and
\item need to make decisions and evaluate results.
\end{itemize}

\sld{Uncertainty}
\begin{itemize}
\item permeates science and decision making
\item in the form of
\begin{subitemize}
\item sampling uncertainty,
\item measurement uncertainty, and
\item modeling uncertainty.
\end{subitemize}
\item The alternative to good statistics is
\begin{subitemize}
\item not no statistics,
\item but bad statistics.
\end{subitemize}
\end{itemize}

\sld{Probability \& Statistics}
\begin{itemize}
\item Probability uses math to quantify uncertainty.
\item Bayesian statistics applies probability theory to
\begin{subitemize}
\item data analysis,
\item prediction \& forecasting,
\item decision theory, and
\item model evaluation.
\end{subitemize}
\end{itemize}

\sld{This talk}
\begin{itemize}
\item How to combine data, theory, and probability to
\begin{subitemize}
\item make calibrated predictions,
\item optimal decisions under uncertainty, and
\item solve the replication crisis.
\end{subitemize}
\item Plus a summary of my current work on
\begin{subitemize}
\item algorithms,
\item methodology, and
\item applications.
\end{subitemize}
\end{itemize}

\mypart{}{Bayesian Inference}

\sld{Notation}
\begin{itemize}
\item Variables
\begin{subitemize}
\item $y$ : data (observed)
\item $\theta$ : parameters (unknown)
\end{subitemize}
\item Probability functions
\begin{subitemize}
\item $p(y, \theta)$ : joint density
\item $p(y \mid \theta)$ : sampling density
{\footnotesize \hfill (likelihood as fun of $\theta$)}
\item $p(\theta)$ : prior (parameter marginal)
\item $p(\theta \mid y)$ : posterior
\item $p(y)$ : evidence (data marginal)
\end{subitemize}
\end{itemize}

\sld{Bayesian inversion}
{\footnotesize
\begin{eqnarray*}
p(\theta \mid y) & = & \frac{p(y, \theta)}{p(y)}
\\[4pt]
& = & \frac{p(y \mid \theta) \cdot p(\theta)}{p(y)}
\\[4pt]
& = & \frac{p(y \mid \theta) \cdot p(\theta)}{\int_{\Theta} p(y, \theta) \, \textrm{d}\theta}
\\[4pt]
& = & \frac{p(y \mid \theta) \cdot p(\theta)}{\int_{\Theta} p(y \mid \theta) \cdot p(\theta) \, \textrm{d}\theta}
\\[10pt]
& \propto &
p(y \mid \theta) \cdot p(\theta).
\end{eqnarray*}
\begin{subitemize}
\item posterior proportional to likelihood times prior
\end{subitemize}
}

\sld{Estimates, events, and predictions}
\begin{itemize}
\item Parameter estimate
$$
\hat{\theta}
\ = \ \mathbb{E}\!\left[\theta \mid y\right]
\ = \
\int_{\Theta} \theta \cdot p(\theta \mid y) \, \textrm{d}\theta
$$
\item Event probability ($A \subseteq \Theta$, \ \ {\footnotesize e.g., $A = \{ \theta : \theta > 0\}$})
$$
\textrm{Pr}[A \mid y]
\ = \ \mathbb{E}\!\left[\textrm{I}_A(\theta) \mid y\right]
\ = \
\int_{\Theta} \textrm{I}_A(\theta) \cdot p(\theta \mid y)
\, \textrm{d}\theta
$$
\item Predictive inference
$$
p(\tilde{y} \mid y)
\ = \ \mathbb{E}\!\left[p(\tilde{y} \mid \theta) \ \middle| \ y\right]
\ = \
\int_{\Theta} p(\tilde{y} \mid \theta) \cdot p(\theta \mid y)
\, \textrm{d} \theta
$$
\end{itemize}

\sld{(Markov chain) Monte Carlo}
\begin{subitemize}
\item Given sample $\theta^{(1)}, \ldots, \theta^{(M)} \sim p(\theta \mid y).$
\item General plug-in expectation calculations,
\begin{eqnarray*}
\mathbb{E}[f(\theta) \mid y]
& = &
\int_{\Theta} \, f(\theta) \cdot p(\theta \mid y) \, \textrm{d}\theta
\\[8pt]
& = &
\lim_{M \rightarrow \infty} \,
\frac{1}{M} \, \sum_{m=1}^M \, f\!\left(\theta^{(m)}\right)
\\[8pt]
& \approx &
\frac{1}{M} \, \sum_{m=1}^M \, f\!\left(\theta^{(m)}\right).
\end{eqnarray*}
\item (MCMC) central limit theorem: finite approx error $\propto \frac{\displaystyle 1}{\displaystyle \sqrt{M}}$
\end{subitemize}

\mypart{Example 1}{Birth Ratio}

\sld{Birth ratio \hfill {\Large (Laplace, 1781)}}
\begin{itemize}
\item Live births in Paris, 1745--1770
\begin{subitemize}
\item $y = 251,527$ male, $N = 493,472$ total
\end{subitemize}
\item Sampling:
$p(y \mid N, \theta)
 = \textrm{binomial}(y \mid N, \theta).$
\item Prior:
$p(\theta)
 = \textrm{uniform}(\theta \mid 0, 1)$
\item Posterior:
$p(\theta \mid y)
 = \textrm{beta}(\theta \mid 1 + y, \ 1 + N - y).$
\item {\small $\textrm{Pr}[\theta \in (0.508, 0.512)] = 0.99$}
\hfill {\footnotesize (estimated male birth rate)}
\item {\small $\textrm{Pr}[\theta > 0.5] \approx 1 - 10^{-42}$}
\hfill {\footnotesize (``morally certain'' more boys)}
\end{itemize}

\sld{Coding in Stan}
\begin{stancode}
transformed data {
  int y = 251527;  int N = 493472;
}
parameters {
  real<lower=0, upper=1> theta;
}
model {
  y ~ binomial(N, theta);
}
generated quantities {
  int<lower=0, upper=1> theta_gt_half = (theta > 0.5);
  int<lower = 0> y_sim = binomial_rng(100, theta);
}
\end{stancode}

\sld{Executing}
%
\begin{codein}
> fit <- stan("laplace.stan")
> print(fit, probs=c(0.005, 0.995), digits=3)
\end{codein}
\begin{codeout}
                     mean se_mean   0.5%   99.5%
theta               0.510   0.000  0.508   0.512
theta_gt_half       1.000     NaN  1.000   1.000
y_sim              50.902   0.081 38.000  64.000
\end{codeout}
%
\begin{subitemize}
\item estimate $\hat{\theta}$ is posterior sample mean for $\theta$;
\item 99\% interval for $\theta$ estimated by sample quantiles
\item $\textrm{Pr}[\theta > 0.5]$ estimated by sample mean of indicator
\item {\texttt y\_sim} estimated forecast for next 100 births
\end{subitemize}

\mypart{Example 2}{Linear Models}

\sld{Logistic regression}
%
\begin{stancode}
 data {
   int<lower = 1> K;
   int<lower = 0> N;
   matrix[N, K] x;                  // predictors
   int<lower=0, upper=1> y[N];      // observations
 }
 parameters {
   vector[K] beta;                  // regression coeffs
 }
 model {
   beta ~ normal(0, 2);            // prior
   y ~ bernoulli_logit(x * beta);  // likelihood
 }
\end{stancode}

\sld{Linear regression with prediction}
%
\begin{stancode}
data {
  int<lower=0> K;
  int<lower=0> N;           int<lower=0> N_tilde;
  matrix[N, K] x;           matrix[N_tilde, K] x_tilde;
  vector[N] y;
}
parameters {
  vector[K] beta;           real<lower=0> sigma;
}
model {
  y ~ normal(x * beta, sigma);
}
generated quantities {
  vector[N_tilde] y_tilde
    = normal_rng(x_tilde * beta, sigma);
}
\end{stancode}

\sld{Time series autoregressive: AR(1)}
%
\begin{stancode}
  data {
    int<lower=0> N;   vector[N] y;
  }
  parameters {
    real alpha;  real beta;  real sigma;
  }
  model {
    y[2:n] ~ normal(alpha + beta * y[1:(n-1)], sigma);
  }
\end{stancode}

\sld{Priors with covariance}
%
\vspace*{-3pt}
\begin{subitemize}
\item $G$ groups w.\ varying slope and intercept; \code{gg[n]} indicates group
\vspace*{-4pt}
\item LKJ prior concentrates around unit correlation
 (${ } \propto \textrm{det}(\Omega)^{\eta - 1}$)
\vspace*{-4pt}
\item Cholesky-factor parameterization for efficiency ($\Omega = \textrm{L}_{\Omega} \cdot \textrm{L}_{\Omega}^{\top}$)
\end{subitemize}
\vspace*{-8pt}
\begin{stancode}
parameters {
  vector[2] beta[G];
  cholesky_factor_corr[2] L_Omega;
  vector<lower = 0>[2] sigma;

model {
  matrix[2, 2] L_Sigma = diag_pre_multiply(sigma, L_Omega);
  sigma ~ normal(0, 2);
  L_Omega ~ lkj_cholesky(4);
  beta ~ multi_normal_cholesky(zeros(2), L_Sigma);

  y ~ bernoulli_logit(... + x .* beta[gg]);
\end{stancode}

\sld{Gaussian process}
%
\vspace*{-5pt}
\begin{stancode}
data {
  int<lower=1> N;  vector[N] x; vector[N] y;
} parameters {
  real<lower=0> eta_sq, inv_rho_sq, sigma_sq;
} transformed parameters {
  real<lower=0> rho_sq; rho_sq = inv(inv_rho_sq);
} model {
  matrix[N,N] Sigma;
  for (i in 1:(N-1)) {
    for (j in (i+1):N) {
      Sigma[i,j] = eta_sq * exp(-rho_sq * square(x[i] - x[j]));
      Sigma[j,i] = Sigma[i,j];
  }}
  for (k in 1:N) Sigma[k,k] = eta_sq + sigma_sq;
  eta_sq, inv_rho_sq, sigma_sq ~ cauchy(0,5);
  y ~ multi_normal(rep_vector(0,N), Sigma);
}
\end{stancode}

\mypart{Example 3}{Population Dynamics}

\sld{Population Dynamics \hfill {\large (Volterra 1926)}}
\begin{itemize}
\item populations at $t$ of prey $u(t)$ \& predator $v(t)$
\item Volterra's mechanistic model
$$
\frac{\textrm{d}}{\textrm{d}t}u = (\alpha - \beta \cdot v) \cdot u
\qquad
\frac{\textrm{d}}{\textrm{d}t}v = (-\gamma + \delta \cdot u) \cdot v
$$
\vspace*{-12pt}
\begin{subitemize}
\item $\alpha$: prey growth rate;  \ $\beta$: predation shrinkage
\item $\gamma$: predator shrinkage; \ $\delta$: predation growth
\end{subitemize}
\end{itemize}

\sld{Analytic solution \hfill {\large (Volterra 1926)}}

\begin{center}
\includegraphics[width=0.6\textwidth]{img/volterra-solutions.jpg}
\end{center}

\sld{Measurement error}
\begin{itemize}
\item $u, v$ are observed
\item $\hat{u}, \hat{v}$ predicted by mechanistic model
\item Independent error proportional to population size
\begin{subitemize}
\item $u_t \sim \textrm{lognormal}(\hat{u}_t, \sigma_1)$
\item $v_t \sim \textrm{lognormal}(\hat{v}_t, \sigma_2)$
\end{subitemize}
\item Weakly informative priors inform scale
\begin{subitemize}
\item $\alpha, \gamma \sim \textrm{normal}(1, 0.5)$; \ \ \
$\beta, \delta \sim \textrm{normal}(0.05, 0.05)$
\item $\sigma_1, \sigma_2 \sim \textrm{lognormal}(-1, 1)$
\end{subitemize}
\end{itemize}

\sld{Hudson's Bay Co.\ pelts \hfill {\large (Hewitt 1921)}}
\begin{center}
\includegraphics[width=0.9\textwidth]{img/hudons-bay-data.png}
\end{center}

\sld{Plotted pelt data}
\begin{center}
\includegraphics[width=0.45\textwidth]{img/lynx-hares-1.png}~\includegraphics[width=0.45\textwidth]{img/lynx-hares-2.png}
\end{center}
\begin{itemize}
\item {\it left)} pelts vs. time \hfill
{\it right)} hare vs. lynx pelts
\\
(line per species) \hfill (over time)
\end{itemize}

\sld{Stan dynamics \& error}
\begin{stancode}
real[] dz_dt(real t, real[] uv, real[] theta,
             real[] theta, real[] x_r, int[] x_i) {
  return { (theta[1] - theta[2] * uv[2]) * uv[1],
           (-theta[3] + theta[4] * uv[1]) * uv[2] };
}
...
real z[T, 2]
  = integrate_ode(dz_dt, z0, t0, sol_ts[1:T], theta,
                  rep_array(0.0, 0), rep_array(0, 0));
...
y[1:T, 1] ~ lognormal(z[1:T, 1], sigma_u);
y[1:T, 2] ~ lognormal(z[1:T, 2], sigma_v);
\end{stancode}

\sld{Measurements \& predictions}
\begin{center}
\includegraphics[width=0.8\textwidth]{img/lotka-volterra-posterior.pdf}
\end{center}

\sld{100 predictive draws}
\begin{center}
\includegraphics[width=0.8\textwidth]{img/lotka-volterra-posterior-time.png}
\end{center}

\mypart{Example 4}
       {Baseball Batting Ability}

\sld{Predicting success {\large (Efron \& Morris 1975)}}
\begin{itemize}
\item Observe number of successes $y_n$ in 50 trials (at bats) per item (player) $n \in 1:N$
\item estimate success in remaining trials
\end{itemize}

\mypart{}{Stan}

\sld{What is Stan?}
%
\begin{itemize}
\item Stan's a domain-specific \myemph{probabilistic programming language}
\item Stan \myemph{program} defines a \myemph{differentiable} probability model
  \begin{subitemize}
  \item declares data and (constrained) parameter variables
  \item defines log posterior (or penalized likelihood)
  \item defines predictive quantities
  \end{subitemize}
\item Stan \myemph{inference} fits model \& makes predictions
  \begin{subitemize}
  \item MCMC for full Bayesian inference
  \item variational and Laplace approximate Bayes
  \end{subitemize}
\end{itemize}


\sld{Availability \& Usage}
\begin{subitemize}
\item \textit{Platforms:} \ Linux, Mac OS X, Windows
\vspace*{-4pt}
\item \textit{Interfaces:} \ R, Python, Julia, MATLAB, Mathematica
\vspace*{-4pt}
\item \textit{Developers (academia \& industry):} 40+ \ {\small (15+ FTEs)}
\vspace*{-4pt}
\item \textit{Users:}\ tens or hundreds of thousands
\vspace*{-4pt}
\item \textit{Companies using:} \ hundreds or thousands
\vspace*{-4pt}
\item \textit{Downloads:}\ millions
\vspace*{-4pt}
\item \textit{User's Group:} \ 3000+ registered; 6000+ non-bot views/day
\vspace*{-4pt}
\item \textit{Books using:} \ 10+
\vspace*{-4pt}
\item \textit{Courses using:} \ 100+
\vspace*{-4pt}
\item \textit{Case studies about:} 100+
\vspace*{-4pt}
\item \textit{Articles using:} \ 3000+
\vspace*{-4pt}
\item \textit{Conferences:} 4 (800+ attendance)
\end{subitemize}

\sld{Some published applications}
%
\begin{itemize}
\item \myemph{Physical sciences}: {\footnotesize
astrophysics, particle physics, chemistry, geology, oceanography,
climatology, biogeochemistry, materials science, $\ldots$
}
\item \myemph{Biological sciences}: {\footnotesize
molecular biology, clinical drug trials, entomology, pharmacology,
toxicology, opthalmology, neurology, genomics, agriculture, botany, fisheries,
epidemiology, population ecology, neurology, psychiatry, $\ldots$
}
\vspace*{-3pt}
\item \myemph{Social sciences}: {\footnotesize
 econometrics (macro and micro), population dynamics, cognitive
 science, psycholinguistics, social networks, political science,
 survey sampling, anthropology, sociology, social work, $\ldots$
}
\vspace*{-3pt}
\item \myemph{Other}: {\footnotesize education, public health,
government, finance, machine learning, logistics, electrical engineering,  transportation, actuarial science, sports, advertising, marketing, $\ldots$}
\end{itemize}

\sld{Why is Stan so Popular?}
\begin{subitemize}
\item \myemph{Community}: large, friendly, helpful, and sharing
\item \myemph{Documentation}:  novice to expert; breadth of fields
\item \myemph{Robustness}:  industrial-strength code; user diagnostics
\item \myemph{Flexibility}:  highly expressive language;  large math lib
\item \myemph{Portability}: popular OS, language, and cloud support
\item \myemph{Extensibility}: developer friendly; derived packages
\item \myemph{Speed}:  $2-\infty$ orders of magnitude faster
\item \myemph{Scalability}:  2+ orders of magnitude more scalable
\item \myemph{Openness}: permissive code and doc licensing
\end{subitemize}

\mypart{}{What Stan Does}

\sld{No-U-turn sampler}
%
\begin{itemize}
\item \myemph{Hamiltonian Monte Carlo} (HMC)
\begin{subitemize}
\item \myemph{Potential Energy}: negative log posterior $\log p(\theta \mid y)$
\item \myemph{Kinetic Energy}: random standard normal each iteration
\end{subitemize}
  %
\item Adapt leapfrog algorithm \myemph{during warmup}
  \vspace*{-4pt}
  \begin{itemize}\small
  \item step size adapted to target acceptance rate
  \item Euclidean metric estimated w.\ sample covariance
  \end{itemize}
  %
\item Adapt leapfrog algorithm  \myemph{during sampling}
  \begin{subitemize}
  \item simulate forward and backward in time until U-turn
  \end{subitemize}
\item Multinomial sample trajectory propto energy; bias furthest
  %
\vfill
\hfill
{\footnotesize (Hoffman and Gelman 2011, 2014; Betancourt 2019)}
\end{itemize}

\sld{NUTS vs.\ Gibbs and Metropolis}
%
\begin{center}
\includegraphics[width=0.9\textwidth]{img/nuts-vs.pdf}
\end{center}
\begin{subitemize}
\item Two dimensions from highly correlated 250-dim normal
\item \myemph{1,000,000 draws} from Metropolis and Gibbs (thinned to 1000)
\item \myemph{1000 draws} from NUTS; 1000 independent draws
\end{subitemize}

\sld{Reverse-mode autodiff (adjoint)}
\begin{itemize}
\item Build up expression graph with values $x$ and adjoints $\overline{x}$
\item Differentiating $f:\mathbb{R}^N \rightarrow \mathbb{R}$ is additional $\mathcal{O}(1)$
\item Set final result $y$'s adjoint $\overline{y} = 1$ and proceed in reverse
\item Example:  scalar $c = \log a$
\begin{subitemize}
\item $\overline{a} \ {+}{=} \ \overline{c} \cdot \frac{\displaystyle 1}{\displaystyle a}$
\end{subitemize}
\item Example: scalar $c = a \cdot b$
\begin{subitemize}
\item $\overline{a} \ {+}{=} \ \overline{c} \cdot b$; \ \ \ \ \
$\overline{b} \ {+}{=} \ \overline{c} \cdot a$
\end{subitemize}
\item Example: matrix $C = A^{-1}$
\begin{subitemize}
\item $\overline{A} \ {+}{=} \ -C^{\top} \cdot \overline{C} \cdot C^{\top}$
\end{subitemize}
\end{itemize}

\sld{Stan's Autodiff vs.\ Alternatives}
%
\vspace*{-4pt}
\begin{itemize}
\item Stan is \myemph{fastest} (and uses least memory)
\begin{subitemize}
\item among open-source C++ alternatives
\end{subitemize}
\end{itemize}
\vspace*{-8pt}
\mbox{ } \hfill \hfill
\includegraphics[width=0.48\textwidth]{img/autodiff-eval-matrix-product-eigen.pdf}
\hfill
\includegraphics[width=0.48\textwidth]{img/autodiff-eval-normal-density.pdf}
\hfill \hfill

\sld{Forward-mode autodiff (tangent)}
\begin{itemize}
\item Build tangents forward with values $x$ and tangents $\dot{x}$
\item Differentiating $f:\mathbb{R} \rightarrow \mathbb{R}^M$ is additional $\mathcal{O}(1)$
\item To differentiate w.r.t.\ $x$, set $\dot{x} = 1$ and work forward
\item Example:  scalar $c = \log a$
\begin{subitemize}
\item $\dot{c} = \dot{a} \cdot \frac{\displaystyle 1}{\displaystyle a}$
\end{subitemize}
\item Example: scalar $c = a \cdot b$
\begin{subitemize}
\item $\dot{c} = \dot{a} \cdot b + \dot{b} \cdot a$
\end{subitemize}
\item Example: matrix $C = A^{-1}$
\begin{subitemize}
\item $\dot{C} = -C \cdot \dot{A} \cdot C$
\end{subitemize}
\end{itemize}

\sld{Higher-order autodiff}
\begin{itemize}
\item Open up new algorithms exploiting curvature
\item Considering log densities $f :\mathbb{R}^N \rightarrow \mathbb{R}$
\item Second order derivatives
\begin{subitemize}
\item nest reverse in forward adds $\mathcal{O}(N)$
\item nest forward in forward $\mathcal{O}(N^2)$
\end{subitemize}
\item Third order derivatives
\begin{subitemize}
\item nest reverse in forward in forward $\mathcal{O}(N^2)$
\item nest forward in forward in forward $\mathcal{O}(N^3)$
\end{subitemize}
\end{itemize}

\mypart{}{Projects}

\sld{Language}
\begin{itemize}
\item Language features
\begin{subitemize}
\item lqmbdas with closure by value
\item ragged arrays
\item sparse matrices
\item tuples \& structures
\item user-defined gradients
\end{subitemize}
\item Blockless language
\item Formal specification and denotational semantics
\end{itemize}

\sld{Algorithms}
\begin{itemize}
\item Asynchronous parallel adapation of MCMC
\item Sequential HMC for filtering (i.e., online learning)
\item Matrix automatic differentiation
\item Parallel automatic differentiation
\begin{subitemize}
\item GPU, multi-core, and multi-machine
\end{subitemize}
\item Higher-order automatic differentiation
\end{itemize}

\sld{Methodology}
\begin{itemize}
\item Bayes vs.\ point estimation \& proper scoring
\item Model/data evaluation via calibration and sharpenss (entropy)
\item Model/data evaluation with uncertain (crowdsourced) data
\item Partially pool performance estimates for multiple comparison
\item Push uncertainty through decision making
\end{itemize}

\sld{Applications (1/3)}
\begin{itemize}
\item Arctic soil-carbon response to environmental change
\begin{subitemize}
\item Kathe Todd-Brown (U.\ Florida, environmental engineering)
\item Sean Schaeffer (U.\ Tennessee, agriculture)
\end{subitemize}
\item Differential expression of gene splice variants
\begin{subitemize}
\item Shuonan Chen (Columbia, biostatistics)
\item Chaolin Zhang (Columbia, genomics)
\end{subitemize}
\end{itemize}

\sld{Applications (2/3)}
\begin{itemize}
\item Virtual database completion for National Election Survey
\begin{subitemize}
\item Andrew Gelman (Columbia, statistics \& political science)
\item Steve Ansolabehere (Harvard, political science))
\end{subitemize}
\item Educational testing and teacher evaluation
\begin{subitemize}
\item Andrew Gelman (Columbia, statistics \& political science)
\item Sophia Rabe-Hesketh (UC Berkeley, education \& biostatistics)
\end{subitemize}
\end{itemize}

\sld{Applications (3/3)}
\begin{itemize}
\item Equilibrium \& rational decision making in auction pricing
\begin{subitemize}
\item Tom Sargent (NYU, economics)
\item Shoshanna Vasserman (Stanford, business)
\item Rachel Meager (LSE, economics)
\end{subitemize}
\item Crowdsourced training \& evaluation in machine learning
\begin{subitemize}
\item Massimo Poesio (U. Essex, computer science)
\item Becky Passonneau (Penn State, computer science)
\end{subitemize}
\end{itemize}

\mypart{}{The End}

\mypart{}{Questions?}

\mypart{}{Answers}

\sld{TensorFlow or PyTorch?}
\begin{itemize}
\item embedded in Python
\item similar scope to Stan's differentiable math library
\item TensorFlow optimizes static autodiff; Pytorch dynamic (like Stan)
\item focused on approximate optimization algorithms
\item optimized for neural networks on distributed architecture
\end{itemize}

\sld{TensorFlow Probability?}
\begin{itemize}
\item embedded in Python
\item similar performance to Stan
\item adds stats functions to TensorFlow
\begin{subitemize}
\item catching up to Stan with growing staffing
\end{subitemize}
\item adds inference algorithms on top
\begin{subitemize}
\item main focus is approximate inference \& deep neural nets
\item recently added static parallel NUTS implementation
\end{subitemize}
\item Edward(2) moribund
\end{itemize}

\sld{PyMC4?}
\begin{itemize}
\item embedded in Python
\item replacing Theano analytic diff with TensorFlow autodiff
\item supported by Google
\item more limited library and control flow than Stan
\item more flexible sampling algorithm combination (e.g., discrete)
\item borrowed Stan's HMC algorithms and diagnostics
\item {advantage} is {embedding of language} as Python API
\item {disadvantage} is {limited flexibility} (not arbitrary Python)
\end{itemize}

\sld{Pyro}
\begin{itemize}
\item embedded in Python
\item based on dynamic PyTorch, so most like Stan
\item main focus approximate inference \& deep neural nets
\item borrowed Stan's HMC algorithms and diagnostics
\end{itemize}

\sld{Discrete parameters?}
\begin{itemize}
\item \myemph{Stan's focus}: tractably \myemph{marginalized}
\begin{subitemize}
\item e.g., state-space models (forward algorithm)
\item \myemph{efficient} in theory due to Rao-Blackwell; in practice by eliminating combinatorics; much better tail behavior
\item marginalizations available wherever you find EM \& MML
\end{subitemize}
\item \myemph{Out of scope}: combinatorially \myemph{intractable}
\begin{subitemize}
\item e.g., selection, clustering, random trees, neural nets
\item optimization is NP-hard or worse
\item thus \myemph{nobody knows how to get right} answer in general
\item some special cases can be fit
\end{subitemize}
\item \myemph{In between}: missing count data
\end{itemize}

\sld{Roll my own sampler?}
\begin{itemize}
\item It's a tricky business, so only as a last resort
\begin{subitemize}
\item specific difficulties arise from non-determinism,
floating point, and failures of geometric ergodicity
\end{subitemize}
\item Sometimes it's the only option
\begin{subitemize}
\item known scalable algorithms require derivatives at first-order or higher
\item carefully validate with simulation-based calibration
\item when it works, let us know, and we can put it in Stan!
\end{subitemize}
\end{itemize}


\end{document}
